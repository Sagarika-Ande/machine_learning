1.🧠 What is Machine Learning (ML)?

Machine Learning means teaching computers to learn from data — instead of giving them step-by-step rules.

In normal programming:
You give rules + data → output

In ML:
You give data + output → model learns rules

<<<<<<< HEAD
DL subset of ML & ML subset of AI

2. Types of Machine Learning?


DL is subset of ML - ML is subset of AI

2. Types of Machine Learning?

Supervised Learning: Trains models on labeled data to predict or classify new, unseen data.
                     Learn from labeled data and predict outputs.
📘 Example: Predict house price based on size.  Data: (size → price)

Unsupervised Learning: Finds patterns or groups in unlabeled data, like clustering or dimensionality reduction.
                      Find hidden structures or group similar data.
📊 Example: Group customers into clusters based on spending habits (no labels), Customer Segmentation, Image Clustering, Fraud  Detection.

Reinforcement Learning: Learns through trial and error to maximize rewards, ideal for decision-making tasks.
                        Learn best actions through feedback.
🎮 Example: A robot learns to walk or a game AI learns to win by getting reward points.


3. Steps to Train a Machine Learning Model?

Step 1: Define the Problem

Decide what you want the model to do.
Example: Predict house prices based on features like size, bedrooms, location.
Type of ML problem: Regression (predict continuous value) or Classification (predict categories).

Step 2: Collect Data

Get data related to your problem.
Data can come from CSV files, databases, or APIs.

Step 3: Explore & Prepare Data (EDA & Preprocessing)

EDA (Exploratory Data Analysis): Understand patterns in data. Example: plot Size vs Price.
Clean Data: Handle missing values, remove duplicates.
Feature Engineering: Convert categorical data into numbers.
                    Normalize/scale values if needed.

Tree-based models don’t need scaling because they split data using thresholds, not distances.
Scaling is important for models that use distances or gradients.

Model Type	                 Examples	                         Needs Scaling?            	Why
Tree-based	            Decision Tree, Random Forest, XGBoost	     ❌              No	Uses thresholds, not distances
Distance-based	            KNN, K-Means, SVM (RBF)	               ✅              Yes	Uses distance metrics
Gradient-based	        Linear/Logistic Regression, Neural Nets	   ✅            Yes	Uses gradients for optimization

Step 4: Split Data

Split data into:
       Training set: 70–80% (for training the model)
       Test/Validation set: 20–30% (to check performance)

Step 5: Choose a Model

Example choices:
Linear Regression → continuous output (house price)
Decision Tree / Random Forest → for complex patterns
Logistic Regression / SVM → for classification problems(catagories)

Step 6: Train the Model

Step 7: Evaluate the Model

Check how well it predicts on test data.
Metrics:
       Regression: MSE, RMSE, R²(continues)
       Classification: Accuracy, Precision, Recall, F1-score(catagories)

Step 8: Improve the Model

Try different models or algorithms.
Tune hyperparameters (learning rate, depth, etc.)
Add more features or data.

Step 9: Save & Use the Model
Once happy with performance, save the model,Load later to make predictions


4. 🌟 What is sklearn?

sklearn stands for Scikit-Learn — it’s one of the most popular Machine Learning libraries in Python.
👉 It makes ML easier — you don’t have to write algorithms from scratch!

It provides ready-to-use tools for:
              Training ML models (like regression, classification, clustering)
              Splitting data into train/test
              Evaluating models (accuracy, R², etc.)
              Doing preprocessing (scaling, encoding, etc.)


5. Types of ML (Based on Learning Style)

There are 3 main types of ML, and different models are used under each type:

Supervised Learning	Model learns from labeled data (input → output known)	
- Linear Regression
- Logistic Regression
- Decision Tree
- Random Forest
- Support Vector Machine (SVM)
- K-Nearest Neighbors (KNN)	
EX: Predict house price, classify fruit type, spam detection

Unsupervised Learning	Model learns from unlabeled data to find patterns	
- K-Means Clustering
- Hierarchical Clustering
- Principal Component Analysis (PCA)
- DBSCAN	
EX: Customer segmentation, anomaly detection, data compression

Reinforcement Learning	Model learns by trial & error with rewards	- Q-Learning
- Deep Q-Network (DQN)
- Policy Gradient Methods
EX: Game AI, robot learning, self-driving cars

5. 💡 Quick visual idea:

Case	              Training Accuracy	          Test Accuracy      	    Description
Underfitting	       Low	                         Low	             Too simple, not learning
Overfitting          High	   	                     Low               Too complex, memorizing
Ideal                High/Good	                 High/Good	         Just right, generalizes well

6. 
Gradient: Tells the model which way to go to reduce error.

Gradient Descent: Takes the steps to actually reach the minimum error.

7.
Term	                    Measures	                  Scope
Loss Function	        Error for 1 example	         Single data point
Cost Function	      Average error over dataset	    Whole dataset

8.
Regression → continuous numbers → MAE, MSE, RMSE, R²
Classification → categories → Accuracy, Precision, Recall, F1, ROC-AUC

9. 
1. R² Score (Coefficient of Determination):
👉 Used to check model accuracy in regression (Linear, Lasso, Ridge, etc.)
Measures how well your regression model explains the variation in the target variable.
Value ranges from 0 to 1 (sometimes negative if model is very bad).

Interpretation:
        R² = 1 → Perfect prediction
        R² = 0 → Model predicts just the mean
        R² < 0 → Model is worse than predicting the mean

Analogy: How much of the “pattern” in the data your model has captured.

2. Bias & Variance

Bias: Error due to wrong assumptions in the model.

      High bias → underfitting (too simple, misses patterns)

Variance: Error due to model being too sensitive to training data.

      High variance → overfitting (memorizes noise, fails on new data)

Analogy:

      High Bias: Shoots arrows far from target → consistently wrong
      High Variance: Shoots arrows all over → sometimes hits, mostly inconsistent
      Goal: Balance bias & variance → ideal model

Variance causes overfitting. Regularization reduces variance to prevent overfitting.

3. Regularization (L1 & L2) - Helps reduce overfitting by penalizing large weights.

L1 Regularization (Lasso):
      Adds penalty = sum of absolute values of weights
      Effect: Can make some weights exactly 0 → feature selection

L2 Regularization (Ridge):
        Adds penalty = sum of squared values of weights
        Effect: Shrinks weights but rarely to 0 → keeps all features

Analogy:  
      L1 = “throw away unimportant features”
      L2 = “shrink weights but keep all features”


8. Machine Learning Algorithms?

Difference Between Linear and Logistic Regression

Feature	                                            Linear Regression	                                Logistic Regression
Type of Problem                         	Regression  (predict continuous values)	             Classification (predict categories)
Output	                                   Any real number (e.g., 10.5, 25000)	                  Probability between 0 and 1
Activation Function	                         None (direct linear output)                         	Sigmoid function
Example Use Case	                        Predicting house price, temperature, etc.	          Predicting spam/not spam, pass/fail, disease/no disease
Loss Function	                              Mean Squared Error (MSE)	                         Binary Cross Entropy (Log Loss)


9. SKLEARN?
scikit-learn (sklearn) is a Python library that helps you easily build, train, and evaluate machine learning models — without having to write complex math or algorithms from scratch.

10.What is ROC Curve?

ROC stands for Receiver Operating Characteristic curve.
It is a graph that shows how well your classification model can distinguish between classes — like yes/no, 1/0, or positive/negative.

It helps us evaluate model performance beyond just accuracy.

AUC (Area Under Curve) = the area under the ROC curve.

Ranges from 0 to 1
Higher = better
AUC = 1.0 → Perfect model
AUC = 0.5 → No better than random guessing

🧩 When do we use ROC Curve?

✅ Use ROC Curve (and AUC) when:

You are doing binary classification (e.g., “disease” vs “no disease”).
Your model outputs probabilities (not just 0 or 1).
You want to compare multiple models on how well they separate classes.

🔹 We split data → to test on unseen data
🔹 We use ROC–AUC → to measure how strong the model’s predictions are (not just correct/incorrect)

11. K-Nearest Neighbors (KNN) in Machine Learning?

K-nearest neighbors (KNN) algorithm is a type of supervised ML algorithm which can be used for both classification as well as regression predictive problems.

For classification problems, the KNN algorithm assigns the test data point to the class that appears most frequently among the k-nearest neighbors. In other words, the class with the highest number of neighbors is the predicted class.

For regression problems, the KNN algorithm assigns the test data point the average of the k-nearest neighbors' values.

🧩 Example Comparison:
KNN Example (Classification):
        Predict if a fruit is an Apple or Orange based on weight and color.

K-Means Example (Clustering):
        Group fruits into clusters based on weight and color — without knowing which is apple/orange.
