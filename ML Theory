1.ðŸ§  What is Machine Learning (ML)?

Machine Learning means teaching computers to learn from data â€” instead of giving them step-by-step rules.

In normal programming:
You give rules + data â†’ output

In ML:
You give data + output â†’ model learns rules

<<<<<<< HEAD
DL subset of ML & ML subset of AI

2. Types of Machine Learning?

=======
DL is subset of ML - ML is subset of AI

2. Types of Machine Learning?
>>>>>>> 21404d92e68e16eababa2e02d4995345943ba412
Supervised Learning: Trains models on labeled data to predict or classify new, unseen data.
                     Learn from labeled data and predict outputs.
ðŸ“˜ Example: Predict house price based on size.  Data: (size â†’ price)

Unsupervised Learning: Finds patterns or groups in unlabeled data, like clustering or dimensionality reduction.
<<<<<<< HEAD
                       Find hidden structures or group similar data.
ðŸ“Š Example: Group customers into clusters based on spending habits (no labels).

Reinforcement Learning: Learns through trial and error to maximize rewards, ideal for decision-making tasks.
                        Learn best actions through feedback.
=======
                      Find hidden structures or group similar data.
ðŸ“Š Example: Group customers into clusters based on spending habits (no labels).

Reinforcement Learning: Learns through trial and error to maximize rewards, ideal for decision-making tasks.
                       Find hidden structures or group similar data.
>>>>>>> 21404d92e68e16eababa2e02d4995345943ba412
ðŸŽ® Example: A robot learns to walk or a game AI learns to win by getting reward points.


3. Steps to Train a Machine Learning Model?

Step 1: Define the Problem

Decide what you want the model to do.
Example: Predict house prices based on features like size, bedrooms, location.
Type of ML problem: Regression (predict continuous value) or Classification (predict categories).

Step 2: Collect Data

Get data related to your problem.
Data can come from CSV files, databases, or APIs.

Step 3: Explore & Prepare Data (EDA & Preprocessing)

EDA (Exploratory Data Analysis): Understand patterns in data. Example: plot Size vs Price.
Clean Data: Handle missing values, remove duplicates.
Feature Engineering: Convert categorical data into numbers.
                    Normalize/scale values if needed.

Tree-based models donâ€™t need scaling because they split data using thresholds, not distances.
Scaling is important for models that use distances or gradients.

Step 4: Split Data

Split data into:
       Training set: 70â€“80% (for training the model)
       Test/Validation set: 20â€“30% (to check performance)

Step 5: Choose a Model

Example choices:
Linear Regression â†’ continuous output (house price)
Decision Tree / Random Forest â†’ for complex patterns
Logistic Regression / SVM â†’ for classification problems

Step 6: Train the Model

Step 7: Evaluate the Model

Check how well it predicts on test data.
Metrics:
       Regression: MSE, RMSE, RÂ²
       Classification: Accuracy, Precision, Recall, F1-score

Step 8: Improve the Model

Try different models or algorithms.
Tune hyperparameters (learning rate, depth, etc.)
Add more features or data.

Step 9: Save & Use the Model
Once happy with performance, save the model,Load later to make predictions


4. ðŸŒŸ What is sklearn?

sklearn stands for Scikit-Learn â€” itâ€™s one of the most popular Machine Learning libraries in Python.
ðŸ‘‰ It makes ML easier â€” you donâ€™t have to write algorithms from scratch!

It provides ready-to-use tools for:
              Training ML models (like regression, classification, clustering)
              Splitting data into train/test
              Evaluating models (accuracy, RÂ², etc.)
              Doing preprocessing (scaling, encoding, etc.)


5. Types of ML (Based on Learning Style)

There are 3 main types of ML, and different models are used under each type:

Supervised Learning	Model learns from labeled data (input â†’ output known)	
- Linear Regression
- Logistic Regression
- Decision Tree
- Random Forest
- Support Vector Machine (SVM)
- K-Nearest Neighbors (KNN)	
EX: Predict house price, classify fruit type, spam detection

Unsupervised Learning	Model learns from unlabeled data to find patterns	
- K-Means Clustering
- Hierarchical Clustering
- Principal Component Analysis (PCA)
- DBSCAN	
EX: Customer segmentation, anomaly detection, data compression

Reinforcement Learning	Model learns by trial & error with rewards	- Q-Learning
- Deep Q-Network (DQN)
- Policy Gradient Methods
EX: Game AI, robot learning, self-driving cars

5. ðŸ’¡ Quick visual idea:

Case	              Training Accuracy	          Test Accuracy      	    Description
Underfitting	       Low	                         Low	             Too simple, not learning
Overfitting          High	   	                     Low               Too complex, memorizing
Ideal                High/Good	                 High/Good	         Just right, generalizes well

6. 
Gradient: Tells the model which way to go to reduce error.

Gradient Descent: Takes the steps to actually reach the minimum error.

7.
Term	                    Measures	                  Scope
Loss Function	        Error for 1 example	         Single data point
Cost Function	      Average error over dataset	    Whole dataset

8.
Regression â†’ continuous numbers â†’ MAE, MSE, RMSE, RÂ²
Classification â†’ categories â†’ Accuracy, Precision, Recall, F1, ROC-AUC

9. 
1. RÂ² Score (Coefficient of Determination):
Measures how well your regression model explains the variation in the target variable.
Value ranges from 0 to 1 (sometimes negative if model is very bad).

Interpretation:
        RÂ² = 1 â†’ Perfect prediction
        RÂ² = 0 â†’ Model predicts just the mean
        RÂ² < 0 â†’ Model is worse than predicting the mean

Analogy: How much of the â€œpatternâ€ in the data your model has captured.

2. Bias & Variance

Bias: Error due to wrong assumptions in the model.

      High bias â†’ underfitting (too simple, misses patterns)

Variance: Error due to model being too sensitive to training data.

      High variance â†’ overfitting (memorizes noise, fails on new data)

Analogy:

      High Bias: Shoots arrows far from target â†’ consistently wrong
      High Variance: Shoots arrows all over â†’ sometimes hits, mostly inconsistent
      Goal: Balance bias & variance â†’ ideal model

3. Regularization (L1 & L2) - Helps reduce overfitting by penalizing large weights.

L1 Regularization (Lasso):
      Adds penalty = sum of absolute values of weights
      Effect: Can make some weights exactly 0 â†’ feature selection

L2 Regularization (Ridge):
        Adds penalty = sum of squared values of weights
        Effect: Shrinks weights but rarely to 0 â†’ keeps all features

Analogy:  
      L1 = â€œthrow away unimportant featuresâ€
      L2 = â€œshrink weights but keep all featuresâ€
